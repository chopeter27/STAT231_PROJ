---
title: "Data Summary"
author: "Andrea and Peter"
date: "11/25/2018"
output: html_document
---

Load necessary packages
```{r setup, include=FALSE, echo = FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2) #dynamic graphics
library(tidyverse) #general tidying
library(tidytext) #text tidying
library(tm) #text mining
library(wordcloud) #wordcloud
library(SnowballC) #stemming
library(readr) #reading in csv file
library(dplyr) #manipulating datasets
library(qdap) #text data manipulation
library(syuzhet) #loading sentiment dictionary to calculate presence of various emotions
library(tidyr) #data tidying
library(wordcloud) #make a wordcloud
library(reshape2) #faceting wordcloud by color
library(rpart) #tree modeling for classification
library(partykit) #tree modeling
library(pROC) #area under the curve
set.seed(123)
```

Basic Data Exploration:
```{r}
#Loading the dataset into the Summary Rmd
fake <- read_csv("fake.csv")
real <- read_csv("Articles.csv")
new_ds <- read_csv("data.csv")
fake_type <- c("fake", "satire", "bias", "bs", "conspiracy", "state", "junksci", "hate")
real_type <- c("sports", "business")

real <- real %>% 
  mutate(binary_type = ifelse(NewsType %in% fake_type, 0, 1)) #now fake = 0 and real = 1
fake <- fake %>% 
  mutate(binary_type = ifelse(type %in% fake_type, 0, 1)) #now fake = 0 and real = 1
new_ds <- new_ds %>% 
  filter(Label == 1)
real <- full_join(real, new_ds, by = c("Heading" = "Headline", "Article" = "Body", "binary_type" = "Label"))
real <- real %>% 
  mutate(id = as.character(seq(1:4564)))
combined <- full_join(fake, real, by = c("text" = "Article", "title" = "Heading", "uuid" = "id", "binary_type" = "binary_type")) %>% 
  select(uuid, binary_type, type, title, text)
```

```{r}
#See how many observations are in each type of fake news
combined %>% 
  group_by(type) %>% 
  summarize(n = n())
typetotals <- combined %>% 
  group_by(type) %>% 
  summarize(n = n())
```

```{r}
#making a tidy_fake dataset where we have the the words in their own column for facilitated data analysis and exploration
tidy_combined <- combined %>%
  group_by(text) %>%
  mutate(linenumber = row_number()) %>%
  ungroup %>%
  unnest_tokens(word, text)
```

```{r}
#what are the most common words for each basic emotion?
#we will use the nrc dictionary so these emotions are the following: anger, fear, anticipation, trust, surprise, sadness, joy, and disgust

#anger
nrc_anger <- get_sentiments("nrc") %>% 
  filter(sentiment == "anger")

tidy_combined %>%
  inner_join(nrc_anger) %>%
  count(word, sort = TRUE)

#fear
nrc_fear <- get_sentiments("nrc") %>% 
  filter(sentiment == "fear")

tidy_combined %>%
  inner_join(nrc_fear) %>%
  count(word, sort = TRUE)

#anticipation
nrc_anticipation <- get_sentiments("nrc") %>% 
  filter(sentiment == "anticipation")

tidy_combined %>%
  inner_join(nrc_anticipation) %>%
  count(word, sort = TRUE)

#trust
nrc_trust <- get_sentiments("nrc") %>% 
  filter(sentiment == "trust")

tidy_combined %>%
  inner_join(nrc_trust) %>%
  count(word, sort = TRUE)

#surprise
nrc_surprise <- get_sentiments("nrc") %>% 
  filter(sentiment == "surprise")

tidy_combined %>%
  inner_join(nrc_surprise) %>%
  count(word, sort = TRUE)

#sadness
nrc_sadness <- get_sentiments("nrc") %>% 
  filter(sentiment == "sadness")

tidy_combined %>%
  inner_join(nrc_sadness) %>%
  count(word, sort = TRUE)

#joy
nrc_joy <- get_sentiments("nrc") %>% 
  filter(sentiment == "joy")

tidy_combined %>%
  inner_join(nrc_joy) %>%
  count(word, sort = TRUE)

#disgust
nrc_disgust <- get_sentiments("nrc") %>% 
  filter(sentiment == "disgust")

tidy_combined %>%
  inner_join(nrc_disgust) %>%
  count(word, sort = TRUE)
```

```{r}
#find net sentiment for each type of fake news documented in the dataset using the bing lexicon
#note that some types, such as bs (>400000), have more corresponding observations than other types, such as fake(<400)
combined_sentiment <- tidy_combined %>%
  inner_join(get_sentiments("bing")) %>%
  count(type, sentiment) %>%
  spread(sentiment, n, fill = 0) %>%
  mutate(sentiment = positive - negative)
combined_sentiment

#plot the sentiment score for each type of fake news
ggplot(combined_sentiment, aes(x = type, y = sentiment)) + geom_col() + labs(title = "Sentiment Score for Each Type of Fake News", x = "Type of Fake News", y = "Sentiment Score")
```

```{r}
#we can also get the sentiment score on a scale of -5 to 5 from the afinn lexicon
afinn <- tidy_combined %>% 
  inner_join(get_sentiments("afinn")) %>% 
  group_by(type) %>% 
  summarise(sentiment = sum(score)) %>% 
  mutate(method = "AFINN")
afinn
```

```{r}
#positive and negative words in nrc lexicon
get_sentiments("nrc") %>% 
     filter(sentiment %in% c("positive", 
                             "negative")) %>% 
  count(sentiment)

#positive and negative words in bing lexicon
get_sentiments("bing") %>% 
  count(sentiment)

#both lexicons have more negative words than positive words, but the bing lexicon has a higher ratio of negative to positive words than the nrc lexon. This coan contribute to the results we see in our data
```

```{r}
#counting most frequently appearing words and which sentiment they correspond to (positive or negative) from the bing lexicon
bing_word_counts <- tidy_combined %>%
  inner_join(get_sentiments("bing")) %>%
  count(word, sentiment, sort = TRUE) %>%
  ungroup()
bing_word_counts

bing_word_counts %>%
  group_by(sentiment) %>%
  top_n(10) %>%
  ungroup() %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, scales = "free_y") +
  labs(y = "Contribution to sentiment",
       x = NULL) +
  coord_flip()
```

```{r}
#wordcloud with most frqeuently appearing words
tidy_combined %>%
  anti_join(stop_words) %>%
  count(word) %>%
  with(wordcloud(words = word, freq = n, max.words = 100, min.freq = 1, random.order=FALSE, rot.per = 0.35, colors = brewer.pal(8, "Dark2")))

#wordcloud faceted into positive and negative with color (blue = negative, red = positive)
tidy_combined %>%
  inner_join(get_sentiments("bing")) %>%
  count(word, sentiment, sort = TRUE) %>%
  acast(word ~ sentiment, value.var = "n", fill = 0) %>%
  comparison.cloud(colors = c("blue", "orange"),
                   max.words = 100)
```

Now, it is time to start the machine learning aspect of the project.
```{r}
#let's use the bing lexicon to add a sentiment column to the tidy_fake dataset
tidy_combined_b <- tidy_combined %>%
  inner_join(get_sentiments("bing"))

#using afinn lexicon to append the sentiment score of each word to tidy_fake 
tidy_combined_a <- tidy_combined_b %>% 
  inner_join(get_sentiments("afinn"))
```

```{r}
#calculate average afinn sentiment score for positive and negative words
tidy_combined_a %>%
  select(score, sentiment) %>%
  group_by(sentiment) %>%
  summarise(n = n(), avgscore = sum(score) / n)
```

```{r}
#categorize article as positive or negative overall
tidy_combined_bin <- tidy_combined_a %>%
  select(uuid, score, binary_type) %>% 
  group_by(uuid) %>%
  summarise(n_words = n(), avgscore = sum(score) / n_words, type = mean(binary_type)) %>%
  mutate(articlesent = ifelse(avgscore < 0, "negative", "positive"))
tidy_combined_bin

tidy_combined_c <- tidy_combined_a %>%
  select(uuid, sentiment, score) %>%
  group_by(uuid) %>%
  summarise(n_positive = sum(sentiment == 'positive'),
            n_negative = sum(sentiment == 'negative'))

tidy_combined_final <- inner_join(tidy_combined_bin, tidy_combined_c, by = c('uuid' = 'uuid')) %>%
  mutate(txt_type = as.factor(type)) %>%
  select(-type)

n <- nrow(tidy_combined_final)
train_id <- sample(1:n, size = round(n * 0.8))
train <- tidy_combined_final[train_id,]
test <- tidy_combined_final[-train_id,]

tree <- rpart(txt_type ~ avgscore + n_words + n_positive + n_negative, data = train)
plot(as.party(tree))
tree

prediction <- predict(tree, test)
test <- test %>% 
  mutate(prediction = prediction)
roc_obj <- roc(test$txt_type, test$prediction)
auc(roc_obj)
plot(roc_obj)
```
Based on this tree, we can see that none of the predictors (average score, number of words, number of positive words, and number of number of negative words) are userful in predicting whether an article is real or fake.

```{r}
#Making a logistic regression model for txt_type
mylogit <- glm(txt_type ~ avgscore + n_words + n_positive, data = tidy_combined_final, family = "binomial")
#Omitted n_negative because this is colinear with n_negative
summary(mylogit)
plot(mylogit)
```


