---
title: "Detecting Fake News Using Sentiment Analysis"
author: "Andrea Boskovic and Peter Cho"
date: "12/18/2018"
output:
  pdf_document:
    latex_engine: xelatex
---

```{r setup, include = TRUE, message = FALSE}
# Loading necessary packages

knitr::opts_chunk$set(echo = TRUE)

# Tidying data packages
library(ggplot2) #dynamic graphics
library(tidyr) #data tidying
library(readr) #reading in csv file
library(dplyr) #manipulating datasets

# Text Data Extraction and Manipulation
library(tm) #text mining
library(tidytext) #text tidying
library(wordcloud) #to make wordcloud
library(reshape2) #to make faceted wordcloud

# Decision Tree Modeling Packages
library(rpart) #tree modeling for classification
library(partykit) #tree modeling
library(pROC) #area under the curve

#Setting the seed for functions involving randomness
set.seed(123)
```

```{r, message = FALSE}
# Loading the dataset
fake <- read_csv("fake.csv") #all fake
real <- read_csv("Articles.csv") #all real
new_ds <- read_csv("data.csv") #combination of real and fake
fake_type <- c("fake", "satire", "bias", "bs", "conspiracy", "state", "junksci", "hate")
real_type <- c("sports", "business")

# Merging the datasets and removing unnecessary columns
real <- real %>% 
  mutate(binary_type = ifelse(NewsType %in% fake_type, 0, 1)) #now fake = 0 and real = 1
fake <- fake %>% 
  mutate(binary_type = ifelse(type %in% fake_type, 0, 1)) #now fake = 0 and real = 1
new_ds <- new_ds %>% 
  filter(Label == 1)
real <- full_join(real, new_ds, by = c("Heading" = "Headline", "Article" = "Body", "binary_type" = "Label"))
real <- real %>% 
  mutate(id = as.character(seq(1:4564))) %>%
  mutate(realtype = "real")

# Making a combined dataset with both fake and real articles and selecting only for the uuid (unique identifier for each article), binary_type (0 for fake and 1 for real), type (type of news: bs, bias, conspiracy, real, etc.), title (title of the article), and text (text of the article) columns.
combined <- full_join(fake, real, by = c("text" = "Article", "title" = "Heading", "uuid" = "id", "binary_type" = "binary_type", "type" = "realtype")) %>% 
  select(uuid, binary_type, type, title, text)

# Making a tidy dataset where we have the the words in their own column for facilitated data analysis and exploration
tidy_combined <- combined %>%
  unnest_tokens(word, text)
head(tidy_combined)
```

```{r}
# Basic Data Exploration:

# This allows us to see how many observations are in each type of fake news.
combined %>% 
  group_by(type) %>% 
  summarize(n = n())
typetotals <- combined %>% 
  group_by(type) %>% 
  summarize(n = n())
typetotals
```

```{r, message = FALSE}
# What are the most common words for each basic emotion?
# We will use the nrc lexicon to categorize each documented word into on of the basic human emotions cataloged in the lexicon: anger, fear, anticipation, trust, surprise, sadness, joy, and disgust.

# Anger
nrc_anger <- get_sentiments("nrc") %>% 
  filter(sentiment == "anger")

tidy_combined %>%
  inner_join(nrc_anger) %>%
  count(word, sort = TRUE) %>%
  head(n = 10)

# Fear
nrc_fear <- get_sentiments("nrc") %>% 
  filter(sentiment == "fear")

tidy_combined %>%
  inner_join(nrc_fear) %>%
  count(word, sort = TRUE) %>%
  head(n = 10)

# Anticipation
nrc_anticipation <- get_sentiments("nrc") %>% 
  filter(sentiment == "anticipation")

tidy_combined %>%
  inner_join(nrc_anticipation) %>%
  count(word, sort = TRUE) %>%
  head(n = 10)

# Trust
nrc_trust <- get_sentiments("nrc") %>% 
  filter(sentiment == "trust")

tidy_combined %>%
  inner_join(nrc_trust) %>%
  count(word, sort = TRUE) %>%
  head(n = 10)

# Surprise
nrc_surprise <- get_sentiments("nrc") %>% 
  filter(sentiment == "surprise")

tidy_combined %>%
  inner_join(nrc_surprise) %>%
  count(word, sort = TRUE) %>%
  head(n = 10)

# Sadness
nrc_sadness <- get_sentiments("nrc") %>% 
  filter(sentiment == "sadness")

tidy_combined %>%
  inner_join(nrc_sadness) %>%
  count(word, sort = TRUE) %>%
  head(n = 10)

# Joy
nrc_joy <- get_sentiments("nrc") %>% 
  filter(sentiment == "joy")

tidy_combined %>%
  inner_join(nrc_joy) %>%
  count(word, sort = TRUE) %>%
  head(n = 10)

# Disgust
nrc_disgust <- get_sentiments("nrc") %>% 
  filter(sentiment == "disgust")

tidy_combined %>%
  inner_join(nrc_disgust) %>%
  count(word, sort = TRUE) %>%
  head(n = 10)
```

```{r, message = FALSE}
# Find net sentiment for each type of fake news documented in the dataset using the bing lexicon. The bing lexicon simply categorizes documented words as "positive" or "negative".
# Note that some types, such as bs (> 400,000), have more corresponding observations than other types, such as fake(< 400)
combined_sentiment <- tidy_combined %>%
  inner_join(get_sentiments("bing")) %>%
  count(type, sentiment) %>%
  spread(sentiment, n, fill = 0) %>%
  mutate(sentiment = positive - negative)
combined_sentiment

# Plot of the sentiment score for each type of news
ggplot(combined_sentiment, aes(x = type, y = sentiment)) + geom_col() + labs(title = "Sentiment Score for All News", x = "Type of News", y = "Sentiment Score")
```

```{r}
# We can also get the sentiment score on a scale of -5 to 5 from the AFINN lexicon. The AFINN lexicon has words documented as -5 being the most negative and 5 being the most positive.
afinn <- tidy_combined %>% 
  inner_join(get_sentiments("afinn")) %>% 
  group_by(type) %>% 
  summarise(sentiment = sum(score)) %>% 
  mutate(method = "AFINN")
head(afinn)
```

```{r}
# It may be useful to investigate the basic contents of the lexicons.

# Positive and negative words in nrc lexicon
get_sentiments("nrc") %>% 
     filter(sentiment %in% c("positive", 
                             "negative")) %>% 
  count(sentiment)

# Positive and negative words in bing lexicon
get_sentiments("bing") %>% 
  count(sentiment)

# Both lexicons have more negative words than positive words, but the bing lexicon has a higher ratio of negative to positive words than the nrc lexicon. This may contribute to the results we see in our data.
```

```{r, message = FALSE}
# Counting the most frequently appearing words and which sentiment they correspond to (positive or negative) from the bing lexicon.
bing_word_counts <- tidy_combined %>%
  inner_join(get_sentiments("bing")) %>%
  count(word, sentiment, sort = TRUE) %>%
  ungroup()
head(bing_word_counts)

bing_word_counts %>%
  group_by(sentiment) %>%
  top_n(10) %>%
  ungroup() %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, scales = "free_y") +
  ggtitle("Positive and Negative Word Frequency") +
  labs(y = "Frequency of Word",
       x = NULL) +
  theme(text = element_text(size=30),
        axis.text.x = element_text(angle=90, hjust=1)) +
  coord_flip()
```

```{r, warning = FALSE, message = FALSE}
# Wordcloud with most frqeuently appearing words
tidy_combined %>%
  anti_join(stop_words) %>%
  count(word) %>%
  with(wordcloud(words = word, freq = n, max.words = 100, min.freq = 1, random.order=FALSE, rot.per = 0.35, colors = brewer.pal(8, "Dark2")))

# Wordcloud faceted into positive and negative with color (blue corresponds to a negative sentiment while red corresponds to a positive sentiment).
tidy_combined %>%
  inner_join(get_sentiments("bing")) %>%
  count(word, sentiment, sort = TRUE) %>%
  acast(word ~ sentiment, value.var = "n", fill = 0) %>%
  comparison.cloud(colors = c("blue", "orange"),
                   max.words = 100)
```

```{r, warning = FALSE, message = FALSE}
# Now, it is time to start the machine learning aspect of the project.
# Using the AFINN lexicon to append the sentiment score of each word to a new dataset called tidy_combined_a.
tidy_combined_a <- tidy_combined %>% 
  inner_join(get_sentiments("afinn"))
```

```{r}
# Categorize article as positive or negative overall based on the average of the AFINN score of the words in the article. Note that we can only categorize words as positive or negative if they are documented in the AFINN lexicon. This is also true for analysis within other lexicons that we have previously completed.
tidy_combined_final <- tidy_combined_a %>%
  select(uuid, score, binary_type) %>% 
  group_by(uuid) %>%
  summarise(n_words = n(), avgscore = sum(score) / n_words, 
            type = mean(binary_type), 
            positive_score = sum(score[score > 0]), 
            negative_score = sum(score[score < 0]),
            n_positive = sum(score > 0),
            n_negative = sum(score < 0)
            ) %>%
  mutate(articlesent = ifelse(avgscore < 0, "Negative", "Positive")) %>% 
  mutate(txt_type = as.factor(type)) %>%
  select(-type)
head(tidy_combined_final)

tidy_combined_final %>% 
  filter(txt_type == 0) %>% 
  summarise(n_negative = n())

# Decision tree training process
n <- nrow(tidy_combined_final)
train_id <- sample(1:n, size = round(n * 0.8))
train <- tidy_combined_final[train_id,]
test <- tidy_combined_final[-train_id,]


tree <- rpart(txt_type ~ avgscore + n_words + n_positive + n_negative + negative_score + positive_score, data = train)
plot(as.party(tree))
tree

prediction <- predict(tree, test)

test <- test %>% 
  mutate(prediction = prediction[1])
roc_obj <- roc(test$txt_type, test$prediction)
auc(roc_obj)
plot(roc_obj)

# Based on this tree, we can see that none of the predictors (average score, number of words, number of positive words, and number of number of negative words) are userful in predicting whether an article is real or fake.
```

```{r, message = FALSE, warning = FALSE}
# Why is this true? Below, we will do some exploration using visualizations to display the poor relationship between our predictions and type of article (real or fake).

# Histogram of Average Sentiment Score by News Type (Real and Fake)
ggplot(tidy_combined_final, aes(x = avgscore, fill = txt_type)) +
  geom_histogram() +
  xlab("Average Score") + 
  ylab("Count") + 
  ggtitle("Histogram of Average Sentiment Score by News Type (Real and Fake)") +
  scale_fill_discrete(name = "Real or Fake News", labels = c("Fake", "Real"))

# Histogram of Number of Words by News Type (Real and Fake)
ggplot(tidy_combined_final, aes(x = n_words, fill = txt_type)) +
  geom_histogram() +
  xlim(0, 500) +
  xlab("Number of Words") + 
  ylab("Count") + 
  ggtitle("Histogram of Number of Words by News Type (Real and Fake)") +
  scale_fill_discrete(name = "Real or Fake News", labels = c("Fake", "Real"))

# Histogram of Number of Negative Words by News Type (Real and Fake)
ggplot(tidy_combined_final, aes(x = n_negative, fill = txt_type)) +
  geom_histogram() +
  xlim(0, 250) +
  xlab("Number of Negative Words") + 
  ylab("Count") + 
  ggtitle("Histogram of Number of Negative Words by News Type (Real and Fake)") +
  scale_fill_discrete(name = "Real or Fake News", labels = c("Fake", "Real"))

# Histogram of Number of Positive Words by News Type (Real and Fake
ggplot(tidy_combined_final, aes(x = n_positive, fill = txt_type)) +
  geom_histogram() +
  xlim(0, 250) +
  xlab("Number of Positive Words") + 
  ylab("Count") + 
  ggtitle("Histogram of Number of Positive Words by News Type (Real and Fake)") +
  scale_fill_discrete(name = "Real or Fake News", labels = c("Fake", "Real"))

# Histogram of Total Negative Score by News Type (Real and Fake)
ggplot(tidy_combined_final, aes(x = negative_score, fill = txt_type)) +
  geom_histogram() +
  xlim(-500, 0) +
  xlab("Total Negative Score") + 
  ylab("Count") + 
  ggtitle("Histogram of Total Negative Score by News Type (Real and Fake)") +
  scale_fill_discrete(name = "Real or Fake News", labels = c("Fake", "Real"))

# Histogram of Total Positive Score by News Type (Real and Fake)
ggplot(tidy_combined_final, aes(x = positive_score, fill = txt_type)) +
  geom_histogram() +
  xlim(0, 500) +
  xlab("Total Positive Score") + 
  ylab("Count") + 
  ggtitle("Histogram of Total Positive Score by News Type (Real and Fake)") +
  scale_fill_discrete(name = "Real or Fake News", labels = c("Fake", "Real"))
```
