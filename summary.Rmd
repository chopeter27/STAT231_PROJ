---
title: "Data Summary"
author: "Andrea and Peter"
date: "11/25/2018"
output: html_document
---

Load necessary packages
```{r setup, include=FALSE, echo = FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2) #dynamic graphics
library(tidyverse) #general tidying
library(tidytext) #text tidying
library(tm) #text mining
library(wordcloud) #wordcloud
library(SnowballC) #stemming
library(readr) #reading in csv file
library(dplyr) #manipulating datasets
library(qdap) #text data manipulation
library(syuzhet) #loading sentiment dictionary to calculate presence of various emotions
```

Basic Exploration:

```{r}
#Loading the dataset into the Summary Rmd
fake <- read_csv("fake.csv")
```

```{r}
#See how many observations are in each type of fake news
fake %>% 
  group_by(type) %>% 
  summarize(n = n())
```

```{r}
#get sentiments for title, thread, text
titlesent <- get_nrc_sentiment(fake$title)
threadsent <- get_nrc_sentiment(fake$thread_title)
textsent <- get_nrc_sentiment(fake$text)

sents <- c(titlesent, threadsent, textsent)
fake <- cbind(fake, sents)
```

```{r}
#make sentiment distributions
#do we want to combine sentiment values of the categories or separate them
fake$positive #refers to the positive count of the title?
head(fake$positive)
```

```{r}
tokens <- data_frame(text = fake$text, uuid = fake$uuid, type = fake$type) %>% #create data frame with text, some ID, and type
  unnest_tokens(word, text) #separate the text into individual strings
tokens %>% 
  inner_join(get_sentiments("bing")) %>% # pull out only sentiment words
  count(sentiment) %>% # count the number of positive & negative words
  spread(sentiment, n, fill = 0) %>% # made data wide rather than narrow
  mutate(sentiment = positive - negative) # number of positive words minus number of negative words 
```

```{r}

```

Project goals: --> I dont think we need this part anymore as per our new goals but I kept it in here just in case --> problems in line 53 and 58 (I agreeee)
```{r}
#remove stopwords
fake_edit <- fake
fake_edit <- fake_edit %>% tm_map(removeWords, stopwords("english"))

#count most frequently appearing words
textfreq <- table(unlist(fake_edit$text))
fake2 <- cbind(names(textfreq), as.integer(textfreq))
dtm <- TermDocumentMatrix(fake2)
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)

#look at first ten most frequent words
head(d, 10)
```

```{r}
#Making a word cloud
set.seed(1234)
wordcloud(words = d$word, freq = d$freq, min.freq = 1,
          max.words = 200, random.order = FALSE, rot.per = 0.35, 
          colors = brewer.pal(8, "Dark2"))
```

